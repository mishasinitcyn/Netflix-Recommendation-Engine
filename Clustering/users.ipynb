{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering (Users)\n",
    "The purpose of this notebook is to cluster the top 10k Netflix users, represented by their preference ratios across the 450 top engineered features.\n",
    "\n",
    "## K-Means\n",
    "We find that the movie feature preferences of the top 10k users follow an elliptical Gaussian distribution for the first 2 principle components, resulting in no valid cluster separation.<br>\n",
    "These results are in stark contrast with the movie feature clusters which have surprisingly well-separated hierarchical cluster groups.\n",
    "\n",
    "## DBSCAN\n",
    "Given the distribution of the user feature vectors, DBSCAN clustering essentially functions as an outlier detection model, resulting in difficult to interpret outliers. Intuitively, outliers are users with rating distributions across the popular features that deviate significantly from the norm among the top 10k users.<br>\n",
    "As an extreme experiment, we perform a very conservative DBSCAN clustering on a 2D PCA projection (explaining only 30% of the variance) of the dataset and find only two outliers: \n",
    "- A user with Anthony Quinn and Donald Pleasence as their two most highly rated features: 60's action fan who loves the Halloween movie series\n",
    "- A user who, despite rating over 800 movies, has not rated a single movie with our engineered features of interest\n",
    "<!-- - Aidan Quinn and Alec Guiness are the most highly rated features for a large portion of the user base\n",
    "- Performing DBSCAN on a 2D PCA projection of the dataset and performing a very conservative DBSCAN clustering, we find two outlier: \n",
    "    1. A user with Anthony Quinn and Donald Pleasence as their two most highly rated features: 60's action fan who loves the Halloween movie series\n",
    "    2. A user who, despite rating over 800 movies, has not rated a single movie with our engineered features of interest -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import plotly.express as px\n",
    "\n",
    "DATA_PATH = \"../data\"\n",
    "NETFLIX_FOLDER_PATH = os.path.join(DATA_PATH, \"netflix_prize\")\n",
    "IMDB_FOLDER_PATH = os.path.join(DATA_PATH, \"imdb\")\n",
    "MIN_OCCURRENCES = 20\n",
    "USER_PROFILE_PATH = os.path.join(DATA_PATH, f\"processed/user_profiles_{MIN_OCCURRENCES}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load User Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(USER_PROFILE_PATH, \"rb\") as f: user_profiles = pickle.load(f)\n",
    "\n",
    "# Convert user profiles to DataFrame\n",
    "user_features = []\n",
    "user_ids = []\n",
    "\n",
    "for user_id, profile in user_profiles.items():\n",
    "    user_ids.append(user_id)\n",
    "    user_features.append([profile[\"feature_preferences\"].get(fid, -1) for fid in range(len(profile[\"feature_preferences\"]))])\n",
    "\n",
    "user_features_df = pd.DataFrame(user_features, index=user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop runtime features 435, 436 437, 438\n",
    "user_features_df = user_features_df.drop(columns=[435, 436, 437, 438])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_feature = pd.read_pickle(os.path.join(DATA_PATH, \"processed\", \"feature_mapping_20.pickle\"))['id_to_feature']\n",
    "\n",
    "def get_top_features(user_id):\n",
    "    preferences = user_profiles[user_id][\"feature_preferences\"]\n",
    "    top = sorted(preferences.items(), key=lambda x: x[1], reverse=True)\n",
    "    # features = [f\"Feature {fid}\" for fid, score in top[:10] if score > 0]\n",
    "    features = [id_to_feature[fid] for fid, score in top[:10] if score > 0]\n",
    "    return \", \".join(features) if features else \"No Preferences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Users in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2 dimensions - using raw data with -1 values intact\n",
    "pca = PCA(n_components=2)\n",
    "user_features_2d = pca.fit_transform(user_features_df)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "plot_df = pd.DataFrame(\n",
    "    user_features_2d,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=user_ids\n",
    ")\n",
    "\n",
    "# Calculate explained variance ratio for axis labels\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "# Create interactive scatter plot with plotly\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x='PC1',\n",
    "    y='PC2',\n",
    "    opacity=0.6,\n",
    "    title='User Feature Preferences - 2D PCA Visualization (Raw Data)',\n",
    "    labels={\n",
    "        'PC1': f'PC1 ({explained_variance[0]:.1f}% variance explained)',\n",
    "        'PC2': f'PC2 ({explained_variance[1]:.1f}% variance explained)'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Update layout for better visibility\n",
    "fig.update_layout(\n",
    "    plot_bgcolor='white',\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Add grid lines\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Print variance explained\n",
    "print(f\"Total variance explained by 2 components: {sum(explained_variance):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Metrics Across Values of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_metrics(data, k_range):\n",
    "    silhouette_scores = []\n",
    "    ch_scores = []\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        silhouette_scores.append(silhouette_score(data, labels))\n",
    "        ch_scores.append(calinski_harabasz_score(data, labels))\n",
    "    \n",
    "    return silhouette_scores, ch_scores\n",
    "\n",
    "# Compute metrics across values of k\n",
    "k_range = range(2, 21)\n",
    "silhouette_scores, ch_scores = compute_clustering_metrics(user_features_df, k_range)\n",
    "\n",
    "# Plot metrics\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.plot(k_range, silhouette_scores, label=\"Silhouette Score\", marker=\"o\")\n",
    "ax1.set_xlabel(\"Number of Clusters (k)\")\n",
    "ax1.set_ylabel(\"Silhouette Score\")\n",
    "ax1.set_title(\"Clustering Metrics for K-Means\")\n",
    "ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(k_range, ch_scores, label=\"Calinski-Harabasz Index\", marker=\"s\", color=\"orange\")\n",
    "ax2.set_ylabel(\"Calinski-Harabasz Index\")\n",
    "ax2.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Clusters for Best k (k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"Selected number of clusters (k): {best_k}\")\n",
    "\n",
    "# Perform K-Means clustering with the best k\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "labels = kmeans.fit_predict(user_features_df)\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "user_features_df[\"cluster\"] = labels\n",
    "\n",
    "# Reduce dimensions to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(user_features_df.drop(\"cluster\", axis=1))\n",
    "reduced_features_df = pd.DataFrame(reduced_features, columns=[\"PCA1\", \"PCA2\"], index=user_features_df.index)\n",
    "reduced_features_df[\"cluster\"] = labels\n",
    "\n",
    "# Add top 3 feature preferences to tooltip\n",
    "def top_3_features(user_id):\n",
    "    preferences = user_profiles[user_id][\"feature_preferences\"]\n",
    "    sorted_prefs = sorted(preferences.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_features = [f\"Feature {fid}\" for fid, score in sorted_prefs[:10] if score > 0]\n",
    "    return \", \".join(top_features) if top_features else \"No Preferences\"\n",
    "\n",
    "reduced_features_df[\"tooltip\"] = reduced_features_df.index.map(top_3_features)\n",
    "\n",
    "# Convert cluster labels to strings for discrete colors\n",
    "reduced_features_df[\"cluster\"] = \"Cluster \" + reduced_features_df[\"cluster\"].astype(str)\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    reduced_features_df,\n",
    "    x=\"PCA1\",\n",
    "    y=\"PCA2\",\n",
    "    color=\"cluster\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1,\n",
    "    hover_data={\"tooltip\": True, \"PCA1\": False, \"PCA2\": False},\n",
    "    title=\"User Clusters Visualized in 2D (PCA)\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7), selector=dict(mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce to 300D via PCA and Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we visualize the first 2 principal components, we observe a linearly separable line around the origin. The two clusters are separated at this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=300)\n",
    "user_features_df.columns = user_features_df.columns.astype(str)\n",
    "user_features_pca = pca.fit_transform(user_features_df)\n",
    "\n",
    "# Compute clustering metrics\n",
    "def compute_clustering_metrics(data, k_range):\n",
    "    metrics = {'silhouette': [], 'ch': []}\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(data)\n",
    "        metrics['silhouette'].append(silhouette_score(data, labels))\n",
    "        metrics['ch'].append(calinski_harabasz_score(data, labels))\n",
    "    return metrics\n",
    "\n",
    "# Find optimal k\n",
    "k_range = range(2, 21)\n",
    "metrics = compute_clustering_metrics(user_features_pca, k_range)\n",
    "best_k = k_range[np.argmax(metrics['silhouette'])]\n",
    "\n",
    "# Perform final clustering\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "labels = kmeans.fit_predict(user_features_pca)\n",
    "\n",
    "# Use first two components from the original PCA\n",
    "reduced_df = pd.DataFrame(user_features_pca[:, :2], columns=['PCA1', 'PCA2'], index=user_features_df.index)\n",
    "reduced_df['cluster'] = [f'Cluster {l}' for l in labels]\n",
    "\n",
    "reduced_df['tooltip'] = reduced_df.index.map(get_top_features)\n",
    "\n",
    "# Visualize\n",
    "fig = px.scatter(\n",
    "    reduced_df,\n",
    "    x=\"PCA1\",\n",
    "    y=\"PCA2\",\n",
    "    color=\"cluster\",\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1,\n",
    "    hover_data={\"tooltip\": True, \"PCA1\": False, \"PCA2\": False},\n",
    "    title=f\"User Clusters in 2D (First Two Components of {best_k} Clusters)\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "fig.show()\n",
    "\n",
    "print(f\"Selected number of clusters (k): {best_k}\")\n",
    "print(f\"Explained variance ratio (300D PCA): {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"Explained variance ratio (first 2 components): {pca.explained_variance_ratio_[:2].sum():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "We cluster with 300D and 2D principal components. The latter, although extreme and only explaining 30% of the variance, ensures that the one true outlier in the dataset (user with no ratings across all of the top features) is flagged as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Calculate k-distances to determine a good value for eps\n",
    "def plot_k_distance(data, k=5):\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(data)\n",
    "    distances, indices = neighbors_fit.kneighbors(data)\n",
    "    distances = np.sort(distances[:, k-1], axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distances)\n",
    "    # plt.axhline(y=0.5, color='r', linestyle='--', label=\"Potential Eps\")\n",
    "    plt.title(\"K-distance Plot (k={})\".format(k))\n",
    "    plt.xlabel(\"Points sorted by distance to {}-th nearest neighbor\".format(k))\n",
    "    plt.ylabel(\"{}-th Nearest Neighbor Distance\".format(k))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_dbscan_clusters(eps, min_samples, dim=2):\n",
    "    # Step 2: Perform DBSCAN Clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    labels = dbscan.fit_predict(user_features_pca)\n",
    "\n",
    "    # Step 3: Identify outliers\n",
    "    outliers = user_features_df.index[labels == -1]\n",
    "    print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "    print(f\"Outlier user IDs: {outliers.tolist()}\")\n",
    "\n",
    "    # Step 4: Visualize results\n",
    "\n",
    "    # Use the first two PCA components for visualization\n",
    "    reduced_df = pd.DataFrame(user_features_pca[:, :2], columns=['PCA1', 'PCA2'], index=user_features_df.index)\n",
    "    reduced_df['cluster'] = labels\n",
    "    reduced_df['cluster'] = reduced_df['cluster'].replace(-1, 'Outlier')\n",
    "\n",
    "    # Add tooltips\n",
    "    reduced_df['tooltip'] = reduced_df.index.map(get_top_features)\n",
    "\n",
    "    # Plot results\n",
    "    fig = px.scatter(\n",
    "        reduced_df,\n",
    "        x='PCA1',\n",
    "        y='PCA2',\n",
    "        color='cluster',\n",
    "        color_discrete_sequence=px.colors.qualitative.Set1,\n",
    "        hover_data={'tooltip': True, 'PCA1': False, 'PCA2': False},\n",
    "        title=f'DBSCAN Clustering Results Using {dim}D Vectors'\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "    fig.show()\n",
    "\n",
    "    # Print a summary\n",
    "    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f\"Number of clusters found (excluding outliers): {num_clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using PCA 300D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=300\n",
    "pca = PCA(n_components=dim)\n",
    "user_features_df.columns = user_features_df.columns.astype(str)\n",
    "user_features_pca = pca.fit_transform(user_features_df.copy())\n",
    "\n",
    "plot_k_distance(user_features_pca, k=5)\n",
    "visualize_dbscan_clusters(10,5,dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster using PCA 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=2\n",
    "pca = PCA(n_components=dim)\n",
    "user_features_df.columns = user_features_df.columns.astype(str)\n",
    "user_features_pca = pca.fit_transform(user_features_df.copy())\n",
    "\n",
    "plot_k_distance(user_features_pca, k=5)\n",
    "visualize_dbscan_clusters(1,5,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
