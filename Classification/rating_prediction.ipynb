{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "1. In this notebook we evaluate classification models in predicting user ratings for movies given movie features and user preference features.<br>\n",
    "Given that our dataset contains millions of ratings from the top 10k users across 9k movies, we sample a subset of ratings at random.<br>\n",
    "\n",
    "2. We also perform hyperparameter tuning on the XGBOOSt model using Randomized Search and evaluate the resulting model performance as well as the learned weights assigned to each feature.\n",
    "\n",
    "- Evaluate Logistic Regression and XGBOOST using K-Fold cross-validation\n",
    "- Print out classification metrics\n",
    "- Visualize Confusion Matrix and ROC-AUC\n",
    "- Tune XGBOOST Hyperparameters using Randomized Search and evaluate\n",
    "- Plot the most and least important learned feature weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    classification_report,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "alt.renderers.enable('default')\n",
    "\n",
    "DATA_PATH = \"../data\"\n",
    "NETFLIX_FOLDER_PATH = os.path.join(DATA_PATH, \"netflix_prize\")\n",
    "IMDB_FOLDER_PATH = os.path.join(DATA_PATH, \"imdb\")\n",
    "MIN_OCCURRENCES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie features\n",
    "MOVIE_FEATURES_PATH = os.path.join(DATA_PATH, f\"processed/movie_features_{MIN_OCCURRENCES}.pickle\")\n",
    "with open(MOVIE_FEATURES_PATH, \"rb\") as f:\n",
    "    movie_features = pickle.load(f)\n",
    "\n",
    "# Load feature mapping\n",
    "FEATURE_MAPPING_PATH = os.path.join(DATA_PATH, f\"processed/feature_mapping_{MIN_OCCURRENCES}.pickle\")\n",
    "with open(FEATURE_MAPPING_PATH, \"rb\") as f:\n",
    "    feature_mapping = pickle.load(f)\n",
    "\n",
    "feature_to_id = feature_mapping['feature_to_id']\n",
    "id_to_feature = feature_mapping['id_to_feature']\n",
    "\n",
    "# Number of features\n",
    "num_features = len(feature_to_id)\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Movie Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of movie IDs and feature IDs\n",
    "movie_ids = list(movie_features.keys())\n",
    "feature_ids = list(id_to_feature.keys())\n",
    "\n",
    "# Create an empty DataFrame\n",
    "movie_feature_matrix = pd.DataFrame(0, index=movie_ids, columns=feature_ids)\n",
    "\n",
    "# Fill the DataFrame\n",
    "for movie_id, features in movie_features.items():\n",
    "    movie_feature_matrix.loc[movie_id, features] = 1\n",
    "\n",
    "print(f\"Movie feature matrix shape: {movie_feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Profiles\n",
    "- Import top 10k users\n",
    "- Randomly sample k users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user profiles\n",
    "USER_PROFILES_PATH = os.path.join(DATA_PATH, f\"processed/user_profiles_{MIN_OCCURRENCES}.pickle\")\n",
    "with open(USER_PROFILES_PATH, \"rb\") as f:\n",
    "    user_profiles = pickle.load(f)\n",
    "\n",
    "# List of user IDs\n",
    "user_ids = list(user_profiles.keys())\n",
    "\n",
    "# Randomly sample k users\n",
    "np.random.seed(RANDOM_SEED)\n",
    "sampled_user_ids = np.random.choice(user_ids, size=k, replace=False)\n",
    "sampled_user_ids_set = set(sampled_user_ids)\n",
    "\n",
    "print(f\"Number of users sampled: {len(sampled_user_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Feature Matrix for Sampled Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter user_profiles to include only sampled users\n",
    "sampled_user_profiles = {user_id: user_profiles[user_id] for user_id in sampled_user_ids}\n",
    "\n",
    "# Create an empty DataFrame for user features\n",
    "user_feature_matrix = pd.DataFrame(-1, index=sampled_user_ids, columns=feature_ids)\n",
    "\n",
    "# Fill the DataFrame with user feature preferences\n",
    "for user_id, user_data in sampled_user_profiles.items():\n",
    "    feature_preferences = user_data['feature_preferences']\n",
    "    for feature_id, preference in feature_preferences.items():\n",
    "        user_feature_matrix.loc[user_id, feature_id] = preference\n",
    "\n",
    "print(f\"User feature matrix shape: {user_feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix-IMDB Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETFLIX_TO_IMDB_PATH = os.path.join(DATA_PATH, \"netflix_to_imdb.json\")\n",
    "with open(NETFLIX_TO_IMDB_PATH, \"r\") as f:\n",
    "    netflix_to_imdb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ratings Data for Sampled Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get list of rating files\n",
    "rating_files = glob.glob(os.path.join(NETFLIX_FOLDER_PATH, \"training_set\", \"*.txt\"))\n",
    "\n",
    "# Set of movie IDs and user IDs we care about\n",
    "movie_ids_set = set(movie_features.keys())\n",
    "user_ids_set = set(sampled_user_ids)  # Use sampled users\n",
    "\n",
    "ratings_data = []\n",
    "total_entries = 0\n",
    "max_entries = 1000000  # Limit the number of entries for the sake of memory\n",
    "\n",
    "for file_path in rating_files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if not lines: continue\n",
    "        # The first line contains the movie ID, ending with ':'\n",
    "        movie_id_line = lines[0].strip()\n",
    "        movie_id = movie_id_line[:-1]  # Remove the colon at the end\n",
    "        # Only process if movie_id is in our set\n",
    "        if movie_id in movie_ids_set:\n",
    "            # Process the rest of the lines\n",
    "            for line in lines[1:]:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) != 3: continue\n",
    "                user_id, rating, date = parts\n",
    "                # Only include sampled users\n",
    "                if user_id in user_ids_set:\n",
    "                    ratings_data.append({\n",
    "                        'movie_id': movie_id,\n",
    "                        'user_id': user_id,\n",
    "                        'rating': int(rating),\n",
    "                        'date': date\n",
    "                    })\n",
    "                    total_entries += 1\n",
    "                    if total_entries >= max_entries: break\n",
    "            if total_entries >= max_entries: break\n",
    "    if total_entries >= max_entries: break\n",
    "\n",
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "print(f\"Total ratings loaded: {ratings_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique users in ratings: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Number of unique movies in ratings: {ratings_df['movie_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binarize Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df['label'] = ratings_df['rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Movie Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index and rename columns for merging\n",
    "movie_feature_matrix.reset_index(inplace=True)\n",
    "movie_feature_matrix.rename(columns={'index': 'movie_id'}, inplace=True)\n",
    "\n",
    "# Rename feature columns to avoid overlap\n",
    "movie_feature_columns = [col for col in movie_feature_matrix.columns if col != 'movie_id']\n",
    "movie_feature_matrix.rename(columns={col: f'movie_feat_{col}' for col in movie_feature_columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare User Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index and rename columns for merging\n",
    "user_feature_matrix.reset_index(inplace=True)\n",
    "user_feature_matrix.rename(columns={'index': 'user_id'}, inplace=True)\n",
    "\n",
    "# Rename feature columns to avoid overlap\n",
    "user_feature_columns = [col for col in user_feature_matrix.columns if col != 'user_id']\n",
    "user_feature_matrix.rename(columns={col: f'user_feat_{col}' for col in user_feature_columns}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ratings with movie features\n",
    "ratings_df = ratings_df.merge(movie_feature_matrix, on='movie_id', how='left')\n",
    "\n",
    "# Merge with user features\n",
    "ratings_df = ratings_df.merge(user_feature_matrix, on='user_id', how='left')\n",
    "print(f\"Data shape after merging: {ratings_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Input Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "movie_feature_cols = [col for col in ratings_df.columns if col.startswith('movie_feat_')]\n",
    "user_feature_cols = [col for col in ratings_df.columns if col.startswith('user_feat_')]\n",
    "\n",
    "# Input features and target variable\n",
    "X = ratings_df[movie_feature_cols + user_feature_cols]\n",
    "y = ratings_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace -1 (indicating missing user preferences) with NaN\n",
    "# X[user_feature_cols] = X[user_feature_cols].replace(-1, np.nan)\n",
    "\n",
    "# # Fill NaN values with 0 (assuming no preference)\n",
    "# X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Cross-validated predictions\n",
    "lr_probas = cross_val_predict(lr, X, y, cv=kf, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "lr_preds = (lr_probas >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "lr_accuracy = accuracy_score(y, lr_preds)\n",
    "lr_precision = precision_score(y, lr_preds)\n",
    "lr_recall = recall_score(y, lr_preds)\n",
    "lr_f1 = f1_score(y, lr_preds)\n",
    "lr_auc = roc_auc_score(y, lr_probas)\n",
    "lr_cm = confusion_matrix(y, lr_preds)\n",
    "\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall: {lr_recall:.4f}\")\n",
    "print(f\"F1 Score: {lr_f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {lr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated predictions\n",
    "xgb_probas = cross_val_predict(xgb, X, y, cv=kf, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "xgb_preds = (xgb_probas >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "xgb_accuracy = accuracy_score(y, xgb_preds)\n",
    "xgb_precision = precision_score(y, xgb_preds)\n",
    "xgb_recall = recall_score(y, xgb_preds)\n",
    "xgb_f1 = f1_score(y, xgb_preds)\n",
    "xgb_auc = roc_auc_score(y, xgb_probas)\n",
    "xgb_cm = confusion_matrix(y, xgb_preds)\n",
    "\n",
    "print(\"XGBoost Metrics:\")\n",
    "print(f\"Accuracy: {xgb_accuracy:.4f}\")\n",
    "print(f\"Precision: {xgb_precision:.4f}\")\n",
    "print(f\"Recall: {xgb_recall:.4f}\")\n",
    "print(f\"F1 Score: {xgb_f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {xgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Logistic Regression Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y, lr_probas)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(lr_fpr, lr_tpr, label=f'ROC Curve (AUC = {lr_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('Logistic Regression ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y, xgb_probas)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(xgb_fpr, xgb_tpr, label=f'ROC Curve (AUC = {xgb_auc:.4f})', color='green')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('XGBoost ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Hyperparameter space\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.5, 0.5),  # From 0.5 to 1.0\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # From 0.5 to 1.0\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'reg_alpha': uniform(0, 0.5),\n",
    "    'reg_lambda': uniform(0.5, 0.5)  # From 0.5 to 1.0\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of parameter settings that are sampled\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Cross-validated predictions\n",
    "best_xgb_probas = cross_val_predict(\n",
    "    best_xgb, X, y, cv=kf, method='predict_proba', n_jobs=-1\n",
    ")[:, 1]\n",
    "best_xgb_preds = (best_xgb_probas >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "best_xgb_accuracy = accuracy_score(y, best_xgb_preds)\n",
    "best_xgb_precision = precision_score(y, best_xgb_preds)\n",
    "best_xgb_recall = recall_score(y, best_xgb_preds)\n",
    "best_xgb_f1 = f1_score(y, best_xgb_preds)\n",
    "best_xgb_auc = roc_auc_score(y, best_xgb_probas)\n",
    "best_xgb_cm = confusion_matrix(y, best_xgb_preds)\n",
    "\n",
    "print(\"Tuned XGBoost Metrics:\")\n",
    "print(f\"Accuracy: {best_xgb_accuracy:.4f}\")\n",
    "print(f\"Precision: {best_xgb_precision:.4f}\")\n",
    "print(f\"Recall: {best_xgb_recall:.4f}\")\n",
    "print(f\"F1 Score: {best_xgb_f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {best_xgb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(best_xgb_cm, annot=True, fmt='d', cmap='Purples')\n",
    "plt.title('Tuned XGBoost Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "best_xgb_fpr, best_xgb_tpr, _ = roc_curve(y, best_xgb_probas)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(best_xgb_fpr, best_xgb_tpr, label=f'ROC Curve (AUC = {best_xgb_auc:.4f})', color='purple')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.title('Tuned XGBoost ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Learned Feature Importance Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learned feature weights\n",
    "importances = best_xgb.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "\n",
    "# Map feature IDs to actual feature names\n",
    "feature_importances['feature'] = feature_importances['feature'].apply(\n",
    "    lambda x: feature_mapping['id_to_feature'][int(x.split('_')[-1])] \n",
    "    if x.split('_')[-1].isdigit() else x\n",
    ")\n",
    "\n",
    "# Sort by importance\n",
    "feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Plot top 20 features\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances.head(20), palette='viridis', ax=ax1)\n",
    "ax1.set_title('Top 20 Most Important Features')\n",
    "ax1.set_xlabel('Importance')\n",
    "ax1.set_ylabel('Feature')\n",
    "\n",
    "# Plot bottom 20 features\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances.tail(20), palette='viridis', ax=ax2)\n",
    "ax2.set_title('20 Least Important Features') \n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_ylabel('Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
