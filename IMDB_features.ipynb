{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Feature Extraction\n",
    "In this notebook we extract movie features from [IMDB datasets](https://datasets.imdbws.com/) to map directly to the Netflix Prize dataset by adapting scripts from [this article by Ilya Grigorik](https://www.igvita.com/2007/01/27/correlating-netflix-and-imdb-datasets/). Each movie is mapped to an array of feature ids, which map to a wide variety of features (crew memnber, genre, country, etc)\n",
    "\n",
    "This notebook produces two pickle files:\n",
    "1. `feature_mapping.pickle` (maps new serialized feature id's to feature)\n",
    "2. `movie_features.pickle` (maps each Netflix movie id to an array of feature id's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import itertools\n",
    "from typing import Dict, Set, Any\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "NETFLIX_FOLDER_PATH = os.path.join(DATA_PATH, \"netflix_prize\")\n",
    "IMDB_FOLDER_PATH = os.path.join(DATA_PATH, \"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "`load_netflix_titles()`\n",
    "- You guessed it, loads netflix movie_titles.txt\n",
    "\n",
    "`load_imdb_data()`\n",
    "- Loads all the IMDB datasets (name, title.basics, title.crew, title.principals, title.akas)\n",
    "- Only keep movies, TV show seasons and episodes, concerts, etc. are out of scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_netflix_titles() -> Dict[int, Dict[str, str]]:\n",
    "    \"\"\"Load Netflix Prize movie titles\"\"\"\n",
    "    netflix_titles = {}\n",
    "    netflix_titles_path = os.path.join(NETFLIX_FOLDER_PATH, \"movie_titles.txt\")\n",
    "    \n",
    "    with open(netflix_titles_path, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            movie_id, year, title = line.strip().split(',', 2)\n",
    "            netflix_titles[int(movie_id)] = {\n",
    "                'title': title,\n",
    "                'year': year\n",
    "            }\n",
    "    return netflix_titles\n",
    "\n",
    "def load_imdb_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load all required IMDB datasets\"\"\"\n",
    "    datasets = {\n",
    "        'names': 'name.basics.tsv.gz',\n",
    "        'titles': 'title.basics.tsv.gz',\n",
    "        'crew': 'title.crew.tsv.gz',\n",
    "        'principals': 'title.principals.tsv.gz'\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    for key, filename in datasets.items():\n",
    "        file_path = os.path.join(IMDB_FOLDER_PATH, filename)\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            data[key] = pd.read_csv(f, sep='\\t', na_values='\\\\N')\n",
    "    \n",
    "    # Filter titles to movies only\n",
    "    data['movies'] = data['titles'][data['titles']['titleType'] == 'movie']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, imdb_data: Dict[str, pd.DataFrame], netflix_to_imdb: Dict[int, str]):\n",
    "        print(\"Initializing FeatureExtractor...\")\n",
    "        self.feature_id_counter = itertools.count(1)\n",
    "        self.feature_to_id: Dict[str, int] = {}\n",
    "        self.feature_counts: Dict[str, int] = {}\n",
    "        \n",
    "        # Get the set of IMDB IDs we actually need\n",
    "        relevant_imdb_ids = set(netflix_to_imdb.values())\n",
    "        print(f\"Filtering data for {len(relevant_imdb_ids)} matched movies...\")\n",
    "        \n",
    "        # Convert startYear to numeric\n",
    "        imdb_data['movies']['startYear'] = pd.to_numeric(imdb_data['movies']['startYear'], errors='coerce')\n",
    "        \n",
    "        # Filter out movies after 2005 and create index\n",
    "        movies_mask = (\n",
    "            (imdb_data['movies']['startYear'] <= 2005) & \n",
    "            (imdb_data['movies']['tconst'].isin(relevant_imdb_ids))\n",
    "        )\n",
    "        self.movies = imdb_data['movies'][movies_mask].set_index('tconst')\n",
    "        print(f\"Filtered movies to {len(self.movies)} entries\")\n",
    "        \n",
    "        # Filter and index crew data\n",
    "        self.crew = imdb_data['crew'][imdb_data['crew']['tconst'].isin(relevant_imdb_ids)].set_index('tconst')\n",
    "        print(f\"Filtered crew data to {len(self.crew)} entries\")\n",
    "        \n",
    "        # Get relevant person IDs from crew data\n",
    "        person_ids = set()\n",
    "        for crew_row in self.crew.itertuples():\n",
    "            for role in ['directors', 'writers']:\n",
    "                if hasattr(crew_row, role) and pd.notna(getattr(crew_row, role)):\n",
    "                    person_ids.update(getattr(crew_row, role).split(','))\n",
    "        \n",
    "        # Filter principals and get additional person IDs\n",
    "        relevant_principals = imdb_data['principals'][\n",
    "            imdb_data['principals']['tconst'].isin(relevant_imdb_ids)\n",
    "        ]\n",
    "        person_ids.update(relevant_principals['nconst'].dropna().unique())\n",
    "        \n",
    "        # Filter and index names data to only relevant people\n",
    "        self.names = imdb_data['names'][imdb_data['names']['nconst'].isin(person_ids)].set_index('nconst')\n",
    "        print(f\"Filtered names data to {len(self.names)} entries\")\n",
    "        \n",
    "        # Create principals lookup dictionary\n",
    "        print(\"Creating principals mappings...\")\n",
    "        self.principals_dict = {}\n",
    "        category_mapping = {\n",
    "            'actor': 'Cast',\n",
    "            'actress': 'Cast',\n",
    "            'producer': 'Producer',\n",
    "            'composer': 'Composer'\n",
    "        }\n",
    "        \n",
    "        for _, row in relevant_principals.iterrows():\n",
    "            if row['category'] in category_mapping:\n",
    "                movie_id = row['tconst']\n",
    "                if movie_id not in self.principals_dict:\n",
    "                    self.principals_dict[movie_id] = []\n",
    "                if pd.notna(row['nconst']):\n",
    "                    self.principals_dict[movie_id].append(\n",
    "                        (category_mapping[row['category']], row['nconst'])\n",
    "                    )\n",
    "        \n",
    "        print(\"FeatureExtractor initialization complete\")\n",
    "    \n",
    "    def get_feature_id(self, feature: str) -> int:\n",
    "        if feature not in self.feature_to_id:\n",
    "            self.feature_to_id[feature] = next(self.feature_id_counter)\n",
    "            self.feature_counts[feature] = 1\n",
    "        else:\n",
    "            self.feature_counts[feature] += 1\n",
    "        return self.feature_to_id[feature]\n",
    "    \n",
    "    def extract_features(self, imdb_id: str) -> Set[int]:\n",
    "        \"\"\"Extract all features for a movie\"\"\"\n",
    "        feature_names = set()\n",
    "        \n",
    "        # Get movie details\n",
    "        if imdb_id not in self.movies.index:\n",
    "            return set()\n",
    "        \n",
    "        movie = self.movies.loc[imdb_id]\n",
    "        \n",
    "        # Process crew\n",
    "        if imdb_id in self.crew.index:\n",
    "            crew_row = self.crew.loc[imdb_id]\n",
    "            for role in ['directors', 'writers']:\n",
    "                if pd.notna(crew_row[role]):\n",
    "                    for person_id in crew_row[role].split(','):\n",
    "                        if person_id in self.names.index:\n",
    "                            name = self.names.loc[person_id, 'primaryName']\n",
    "                            if pd.notna(name):\n",
    "                                feature = f\"{role.title()[:-1]}:{name}\"\n",
    "                                feature_names.add(feature)\n",
    "        \n",
    "        # Process principals\n",
    "        if imdb_id in self.principals_dict:\n",
    "            for category, person_id in self.principals_dict[imdb_id]:\n",
    "                if person_id in self.names.index:\n",
    "                    name = self.names.loc[person_id, 'primaryName']\n",
    "                    if pd.notna(name):\n",
    "                        feature = f\"{category}:{name}\"\n",
    "                        feature_names.add(feature)\n",
    "        \n",
    "        # Process genres\n",
    "        if pd.notna(movie['genres']):\n",
    "            for genre in movie['genres'].split(','):\n",
    "                feature = f\"Genre:{genre}\"\n",
    "                feature_names.add(feature)\n",
    "        \n",
    "        # Process decade\n",
    "        if not np.isnan(movie['startYear']):\n",
    "            decade = (int(movie['startYear']) // 10) * 10\n",
    "            feature = f\"Decade:{decade}s\"\n",
    "            feature_names.add(feature)\n",
    "        \n",
    "        # Process runtimeMinutes\n",
    "        if pd.notna(movie['runtimeMinutes']):\n",
    "            runtime = pd.to_numeric(movie['runtimeMinutes'], errors='coerce')\n",
    "            if not np.isnan(runtime):\n",
    "                # Quantize runtime into bins\n",
    "                if runtime < 60:\n",
    "                    runtime_bin = \"<60min\"\n",
    "                elif runtime < 90:\n",
    "                    runtime_bin = \"60-90min\"\n",
    "                elif runtime < 120:\n",
    "                    runtime_bin = \"90-120min\"\n",
    "                else:\n",
    "                    runtime_bin = \">120min\"\n",
    "                feature = f\"Runtime:{runtime_bin}\"\n",
    "                feature_names.add(feature)\n",
    "        \n",
    "        # # Process isAdult\n",
    "        # if pd.notna(movie['isAdult']):\n",
    "        #     is_adult = pd.to_numeric(movie['isAdult'], errors='coerce')\n",
    "        #     if not np.isnan(is_adult):\n",
    "        #         feature = f\"isAdult:{int(is_adult)}\"\n",
    "        #         feature_names.add(feature)\n",
    "        \n",
    "        # Now, for each unique feature, get its ID and increment the count once\n",
    "        features = set()\n",
    "        for feature in feature_names:\n",
    "            features.add(self.get_feature_id(feature))\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process decade\n",
    "# runtime = pd.to_numeric(imdb_data['movies'].iloc[0]['runtimeMinutes'])\n",
    "# not np.isnan(runtime)\n",
    "# # if not np.isnan(movie['startYear']):\n",
    "# #     decade = (int(movie['startYear']) // 10) * 10\n",
    "# #     feature = f\"Decade:{decade}s\"\n",
    "# #     features.add(self.get_feature_id(feature))\n",
    "\n",
    "# # # Process runtimeMinutes\n",
    "# # if pd.notna(movie['runtimeMinutes']):\n",
    "# #     runtime = pd.to_numeric(movie['runtimeMinutes'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(title):\n",
    "    \"\"\"Clean and validate a title string\"\"\"\n",
    "    if pd.isna(title) or not isinstance(title, str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    title = str(title).lower().strip()\n",
    "    \n",
    "    # Check if title contains any alphabetic characters\n",
    "    if not any(c.isalpha() for c in title):\n",
    "        return None\n",
    "        \n",
    "    return title\n",
    "\n",
    "def match_titles(netflix_titles: Dict[int, Dict[str, str]], \n",
    "                 imdb_movies: pd.DataFrame,\n",
    "                 similarity_threshold: int = 90) -> Dict[int, str]:\n",
    "    \"\"\"Match Netflix titles to IMDB titles using optimized fuzzy matching\"\"\"\n",
    "    print(\"Preparing dataframes...\")\n",
    "    \n",
    "    # Convert Netflix titles to DataFrame\n",
    "    netflix_df = pd.DataFrame.from_dict(netflix_titles, orient='index')\n",
    "    netflix_df['year'] = pd.to_numeric(netflix_df['year'], errors='coerce')\n",
    "    netflix_df = netflix_df.reset_index().rename(columns={'index': 'netflix_id'})\n",
    "    \n",
    "    # Clean Netflix titles\n",
    "    netflix_df['clean_title'] = netflix_df['title'].apply(clean_title)\n",
    "    netflix_df = netflix_df.dropna(subset=['clean_title'])\n",
    "    print(f\"Netflix movies (after cleaning): {len(netflix_df)} of {len(netflix_titles)} total\")\n",
    "    \n",
    "    # Clean up IMDB movies DataFrame\n",
    "    imdb_df = imdb_movies.copy()\n",
    "    imdb_df['startYear'] = pd.to_numeric(imdb_df['startYear'], errors='coerce')\n",
    "    imdb_df['clean_title'] = imdb_df['primaryTitle'].apply(clean_title)\n",
    "    imdb_df = imdb_df.dropna(subset=['clean_title'])\n",
    "    print(f\"IMDB movies (after cleaning): {len(imdb_df)} of {len(imdb_movies)} total\")\n",
    "    \n",
    "    netflix_to_imdb = {}\n",
    "    total = len(netflix_df)\n",
    "    \n",
    "    print(\"Starting matching process...\")\n",
    "    \n",
    "    # Match each Netflix movie id to an IMDB movie id via fuzzy match\n",
    "    for idx, netflix_row in netflix_df.iterrows():\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Processed {idx}/{total} titles ({(idx/total)*100:.1f}%)\")\n",
    "        \n",
    "        netflix_id = netflix_row['netflix_id']\n",
    "        netflix_year = netflix_row['year']\n",
    "        netflix_title = netflix_row['clean_title']\n",
    "        \n",
    "        # Filter IMDB movies by year (±1 year) for faster lookup\n",
    "        if pd.notna(netflix_year):\n",
    "            year_mask = (imdb_df['startYear'] >= netflix_year - 1) & \\\n",
    "                        (imdb_df['startYear'] <= netflix_year + 1)\n",
    "            candidates = imdb_df[year_mask]\n",
    "        else:\n",
    "            candidates = imdb_df\n",
    "        \n",
    "        # Find best match among candidates\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for _, imdb_row in candidates.iterrows():\n",
    "            imdb_title = imdb_row['clean_title']\n",
    "            score = fuzz.ratio(netflix_title, imdb_title)\n",
    "            \n",
    "            if score > best_score and score >= similarity_threshold:\n",
    "                best_score = score\n",
    "                best_match = imdb_row['tconst']\n",
    "        \n",
    "        if best_match:\n",
    "            netflix_to_imdb[netflix_id] = best_match\n",
    "            if idx % 100 == 0:\n",
    "                matched_title = imdb_df[imdb_df['tconst'] == best_match]['primaryTitle'].iloc[0]\n",
    "                print(f\"Matched: {netflix_row['title']} -> {matched_title} (score: {best_score})\")\n",
    "    \n",
    "    print(f\"\\nMatching complete. Found {len(netflix_to_imdb)} matches.\")\n",
    "    return netflix_to_imdb\n",
    "\n",
    "def print_sample_matches(netflix_titles, imdb_movies, netflix_to_imdb, n=5):\n",
    "    \"\"\"Print n sample matches to verify matching quality\"\"\"\n",
    "    print(\"\\nSample matches:\")\n",
    "    samples = list(netflix_to_imdb.items())[:n]\n",
    "    \n",
    "    for netflix_id, imdb_id in samples:\n",
    "        netflix_info = netflix_titles[netflix_id]\n",
    "        imdb_info = imdb_movies[imdb_movies['tconst'] == imdb_id].iloc[0]\n",
    "        print(f\"\\nNetflix: {netflix_info['title']} ({netflix_info['year']})\")\n",
    "        print(f\"IMDB:    {imdb_info['primaryTitle']} ({imdb_info['startYear']})\")\n",
    "        print(f\"IMDB ID: {imdb_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "netflix_titles = load_netflix_titles()\n",
    "imdb_data = load_imdb_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here the netflix dataset only goes up to 2005 so we will then prune out all >2005 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(int(netflix_titles[entry]['year']) for entry in netflix_titles if netflix_titles[entry]['year'].isdigit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create mapping between Netflix and IMDB movie id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute netflix to imdb id mapping\n",
    "# print(\"Matching titles...\")\n",
    "# netflix_to_imdb = match_titles(netflix_titles, imdb_data['movies'])\n",
    "# with open('data/netflix_to_imdb.json', 'w') as f: json.dump(netflix_to_imdb, f)  \n",
    "# print_sample_matches(netflix_titles, imdb_data['movies'], netflix_to_imdb)\n",
    "# print(f\"Matched {len(netflix_to_imdb)} movies\")\n",
    "\n",
    "# import precomputed mapping\n",
    "with open('data/netflix_to_imdb.json', 'r') as f: netflix_to_imdb = json.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the id mappings to extract features for each movie (cast member, country, genre, etc) from the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting features...\")\n",
    "extractor = FeatureExtractor(imdb_data, netflix_to_imdb)\n",
    "movie_features = {}\n",
    "\n",
    "total = len(netflix_to_imdb)\n",
    "for idx, (netflix_id, imdb_id) in enumerate(netflix_to_imdb.items(), 1):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing movie {idx}/{total} ({idx/total*100:.1f}%)\")\n",
    "    features = extractor.extract_features(imdb_id)\n",
    "    if features:\n",
    "        movie_features[netflix_id] = features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.feature_counts['Runtime:90-120min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune Features\n",
    "- Remove features below 20 occurrences\n",
    "- Assign new sequential id's to the features instead of using IMDB id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_OCCURRENCES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pruning features...\")\n",
    "features_to_keep = {\n",
    "    feature for feature, count in extractor.feature_counts.items() \n",
    "    if count >= MIN_OCCURRENCES\n",
    "}\n",
    "\n",
    "# Rebuild the feature IDs with new sequential IDs\n",
    "feature_to_id = {\n",
    "    feature: idx for idx, feature in enumerate(sorted(features_to_keep), start=1)\n",
    "}\n",
    "\n",
    "pruned_features = {}\n",
    "for netflix_id, old_feature_ids in movie_features.items():\n",
    "    # Map old feature IDs to their feature names and then to new IDs\n",
    "    kept_features = {\n",
    "        feature_to_id[feature_name]  # Get new ID\n",
    "        for feature_name, old_id in extractor.feature_to_id.items()  # Get name from old ID\n",
    "        if feature_name in features_to_keep and old_id in old_feature_ids  # Check both conditions\n",
    "    }\n",
    "    if kept_features:\n",
    "        pruned_features[netflix_id] = sorted(kept_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.join(DATA_PATH, \"processed\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving results...\")\n",
    "with open(os.path.join(output_dir, f'feature_mapping_{MIN_OCCURRENCES}.pickle'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'feature_to_id': feature_to_id,\n",
    "        'id_to_feature': {v: k for k, v in feature_to_id.items()}\n",
    "    }, f)\n",
    "\n",
    "with open(os.path.join(output_dir, f'movie_features_{MIN_OCCURRENCES}.pickle'), 'wb') as f:\n",
    "    pickle.dump(pruned_features, f)\n",
    "\n",
    "print(f\"Done! Processed {len(pruned_features)} movies with {len(feature_to_id)} features\")\n",
    "print(f\"Results saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Feature Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feature_frequencies(extractor, feature_to_id, output_dir):\n",
    "    feature_frequencies = {}\n",
    "    for feature_name, count in extractor.feature_counts.items():\n",
    "        if feature_name in feature_to_id:\n",
    "            new_id = feature_to_id[feature_name]\n",
    "            feature_frequencies[new_id] = count\n",
    "\n",
    "    frequencies_path = os.path.join(output_dir, f'feature_frequencies_{MIN_OCCURRENCES}.pickle')\n",
    "    with open(frequencies_path, 'wb') as f:\n",
    "        pickle.dump(feature_frequencies, f)\n",
    "\n",
    "# Save feature frequencies\n",
    "save_feature_frequencies(extractor, feature_to_id, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping = pd.read_pickle(f'data/processed/feature_mapping_{MIN_OCCURRENCES}.pickle')\n",
    "# movie_features = pd.read_pickle(f'data/processed/movie_features_{MIN_OCCURRENCES}.pickle')\n",
    "\n",
    "# Map the most popular feature IDs to their features and print\n",
    "feature_frequencies = pd.read_pickle(f'data/processed/feature_frequencies_{MIN_OCCURRENCES}.pickle')\n",
    "sorted_features = sorted(feature_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "id_to_feature = {v: k for k, v in feature_to_id.items()}\n",
    "\n",
    "for feature_id, count in sorted_features[:10]:  # Print top 10 features\n",
    "    print(f\"{id_to_feature[feature_id]}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
